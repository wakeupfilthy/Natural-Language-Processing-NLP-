{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 4. Similitud de palabras y oraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar cuerpo de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import re\n",
    "\n",
    "carpeta_libros = \"Libros\"\n",
    "\n",
    "# Leer todos los libros en la carpeta y guardarlos en una lista\n",
    "documentos = []\n",
    "for nombre_archivo in os.listdir(carpeta_libros):\n",
    "    if nombre_archivo.endswith('.txt'):\n",
    "        with open(os.path.join(carpeta_libros, nombre_archivo), 'r', encoding='utf-8') as archivo:\n",
    "            documentos.append(archivo.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dreams - Henri Bergson.txt',\n",
       " 'Memoirs of Extraordinary Popular Delusions and the Madness of Crowds - Charles Mackay.txt',\n",
       " 'Nerves and Common Sense - Annie Payson Call.txt',\n",
       " 'Ten Thousand Dreams Interpreted - Gustavus Hindman Miller.txt',\n",
       " 'The Mind and the Brain - Alfred Binet.txt',\n",
       " 'The Mind of the Child - William T. Preyer.txt',\n",
       " 'The Science of Human Nature A Psychology for Beginners - William Henry Pyle.txt',\n",
       " 'The Trained Memory - Warren Hilton.txt',\n",
       " 'The Untroubled Mind - Herbert J. Hall.txt',\n",
       " 'Unconscious Memory - Samuel Butler.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "# Crear el corpus\n",
    "corpus = PlaintextCorpusReader(carpeta_libros, '.*\\.txt')\n",
    "\n",
    "file_ids = corpus.fileids()\n",
    "file_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de preprocesamiento\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stopwords]\n",
    "\n",
    "def lematizar(text):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "def tokenizar(text):\n",
    "    lowercase_text = text.lower()\n",
    "    cleaned_text = re.sub(r'[^a-z]', ' ', lowercase_text) # Eliminar caracteres no alfabéticos\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) # Eliminar espacios en blanco\n",
    "    \n",
    "    sent_text = nltk.sent_tokenize(cleaned_text) # Obtener oraciones\n",
    "    all_tagged_text = []\n",
    "    # iterar sobre cada oración y preprocesarla\n",
    "    for sentence in sent_text:\n",
    "        tokenized_text = nltk.word_tokenize(sentence)\n",
    "        tokenized_text = remove_stopwords(tokenized_text)\n",
    "        lemma_text = lematizar(tokenized_text)\n",
    "        tagged_text = nltk.pos_tag(lemma_text)\n",
    "        all_tagged_text.extend(tagged_text)\n",
    "    return all_tagged_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de Palabras con synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulitud de verbos con métrica \"path_similarity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dreams - Henri Bergson.txt\n",
      "Most common verb:  [('dreaming', 4)]\n",
      "[('dream', 1.0), ('felt', 0.3333333333333333), ('thought', 0.3333333333333333), ('found', 0.3333333333333333), ('seen', 0.3333333333333333)] \n",
      "\n",
      "Memoirs of Extraordinary Popular Delusions and the Madness of Crowds - Charles Mackay.txt\n",
      "Most common verb:  [('made', 30)]\n",
      "[('held', 1.0), ('established', 1.0), ('take', 1.0), ('created', 1.0), ('threw', 1.0)] \n",
      "\n",
      "Nerves and Common Sense - Annie Payson Call.txt\n",
      "Most common verb:  [('come', 3)]\n",
      "[('see', 0.5), ('grow', 0.5), ('seeing', 0.5), ('form', 0.3333333333333333), ('assured', 0.3333333333333333)] \n",
      "\n",
      "Ten Thousand Dreams Interpreted - Gustavus Hindman Miller.txt\n",
      "Most common verb:  [('thought', 38)]\n",
      "[('mean', 1.0), ('intended', 1.0), ('thinking', 1.0), ('recall', 1.0), ('think', 1.0)] \n",
      "\n",
      "The Mind and the Brain - Alfred Binet.txt\n",
      "Most common verb:  [('thought', 5)]\n",
      "[('mean', 1.0), ('supposed', 1.0), ('consider', 1.0), ('meaning', 1.0), ('suppose', 1.0)] \n",
      "\n",
      "The Mind of the Child - William T. Preyer.txt\n",
      "Most common verb:  [('made', 15)]\n",
      "[('established', 1.0), ('take', 1.0), ('gave', 1.0), ('give', 1.0), ('getting', 1.0)] \n",
      "\n",
      "The Science of Human Nature A Psychology for Beginners - William Henry Pyle.txt\n",
      "Most common verb:  [('know', 20)]\n",
      "[('living', 1.0), ('live', 1.0), ('known', 1.0), ('take', 0.5), ('taking', 0.5)] \n",
      "\n",
      "The Trained Memory - Warren Hilton.txt\n",
      "Most common verb:  [('discovered', 8)]\n",
      "[('reveal', 1.0), ('learned', 1.0), ('seeing', 1.0), ('found', 1.0), ('seen', 1.0)] \n",
      "\n",
      "The Untroubled Mind - Herbert J. Hall.txt\n",
      "Most common verb:  [('worry', 9)]\n",
      "[('concerned', 1.0), ('fear', 0.5), ('mean', 0.3333333333333333), ('become', 0.3333333333333333), ('failed', 0.3333333333333333)] \n",
      "\n",
      "Unconscious Memory - Samuel Butler.txt\n",
      "Most common verb:  [('darwin', 12)]\n",
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "# Funcion para obtener los synsets de un verbo\n",
    "def obtener_verbo_synsets(verb):\n",
    "    return wn.synsets(verb, pos=wn.VERB)\n",
    "# Funcion para calcular la similitud entre dos verbos\n",
    "def calcular_similitud_path(verb1, verb2):\n",
    "    synsets1 = obtener_verbo_synsets(verb1)\n",
    "    synsets2 = obtener_verbo_synsets(verb2)\n",
    "    max_similarity = 0 # Similaridad máxima\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            similarity = synset1.path_similarity(synset2) # Calcular la similitud entre los synsets\n",
    "            if similarity and similarity > max_similarity: \n",
    "                max_similarity = similarity # Actualizar la similaridad máxima\n",
    "    return max_similarity\n",
    "# Funcion para encontrar los verbos más similares a un verbo dado\n",
    "def encontrar_verbos_similares(most_common_verb, verbos):\n",
    "    similaridades = [] # Lista de similaridades\n",
    "    for verbo in set(verbos):\n",
    "        if verbo != most_common_verb:\n",
    "            similarity = calcular_similitud_path(most_common_verb, verbo) # Calcular la similitud\n",
    "            if similarity:  # Ignorar similaridades None\n",
    "                similaridades.append((verbo, similarity)) \n",
    "    similaridades.sort(key=lambda x: x[1], reverse=True) # Ordenar por similaridad\n",
    "    return similaridades[:5] # Devolver los 5 verbos más similares\n",
    "# Iterar sobre los documentos\n",
    "for doc in corpus.fileids():\n",
    "    print(doc)\n",
    "    verbos = []\n",
    "    text = corpus.raw(doc)\n",
    "    tagged_text = tokenizar(text)\n",
    "    #Obtener los verbos\n",
    "    for word, tag in tagged_text: \n",
    "        if tag == 'VB' or tag == 'VBD' or tag == 'VBG' or tag == 'VBN' or tag == 'VBP' or tag == 'VBZ':\n",
    "            verbos.append(word)\n",
    "    #Obtener el verbo más común\n",
    "    counter = Counter(verbos)\n",
    "    most_common_verb = counter.most_common(1)\n",
    "    print(\"Most common verb: \",most_common_verb)\n",
    "    #Obtener los synsets del verbo más común\n",
    "    verbos_similares= encontrar_verbos_similares(most_common_verb[0][0], verbos)\n",
    "    print(verbos_similares, \"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud de sustantivos con métrica 'path_similarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dreams - Henri Bergson.txt\n",
      "Most common noun:  [('dream', 20)]\n",
      "[('ambition', 1.0), ('imagery', 0.5), ('desire', 0.5), ('vision', 0.5), ('state', 0.3333333333333333)] \n",
      "\n",
      "Memoirs of Extraordinary Popular Delusions and the Madness of Crowds - Charles Mackay.txt\n",
      "Most common noun:  [('law', 124)]\n",
      "[('police', 1.0), ('philosophy', 0.5), ('posse', 0.5), ('force', 0.5), ('collection', 0.5)] \n",
      "\n",
      "Nerves and Common Sense - Annie Payson Call.txt\n",
      "Most common noun:  [('habit', 7)]\n",
      "[('change', 0.25), ('act', 0.2), ('mind', 0.2), ('power', 0.2), ('turn', 0.2)] \n",
      "\n",
      "Ten Thousand Dreams Interpreted - Gustavus Hindman Miller.txt\n",
      "Most common noun:  [('dream', 171)]\n",
      "[('aspiration', 1.0), ('reverie', 0.5), ('vision', 0.5), ('imagination', 0.5), ('desire', 0.5)] \n",
      "\n",
      "The Mind and the Brain - Alfred Binet.txt\n",
      "Most common noun:  [('mind', 15)]\n",
      "[('idea', 1.0), ('opinion', 0.5), ('content', 0.3333333333333333), ('place', 0.3333333333333333), ('position', 0.25)] \n",
      "\n",
      "The Mind of the Child - William T. Preyer.txt\n",
      "Most common noun:  [('child', 81)]\n",
      "[('issue', 0.5), ('individual', 0.5), ('infant', 0.5), ('person', 0.5), ('balance', 0.3333333333333333)] \n",
      "\n",
      "The Science of Human Nature A Psychology for Beginners - William Henry Pyle.txt\n",
      "Most common noun:  [('psychology', 48)]\n",
      "[('science', 0.5), ('study', 0.3333333333333333), ('memory', 0.3333333333333333), ('field', 0.3333333333333333), ('chemistry', 0.25)] \n",
      "\n",
      "The Trained Memory - Warren Hilton.txt\n",
      "Most common noun:  [('experience', 11)]\n",
      "[('taste', 0.5), ('life', 0.5), ('flash', 0.5), ('time', 0.5), ('vision', 0.5)] \n",
      "\n",
      "The Untroubled Mind - Herbert J. Hall.txt\n",
      "Most common noun:  [('life', 22)]\n",
      "[('spirit', 1.0), ('living', 1.0), ('soul', 0.5), ('time', 0.5), ('existence', 0.5)] \n",
      "\n",
      "Unconscious Memory - Samuel Butler.txt\n",
      "Most common noun:  [('darwin', 31)]\n",
      "[('naturalist', 0.5), ('wallace', 0.3333333333333333), ('lamarck', 0.3333333333333333), ('huxley', 0.25), ('eye', 0.2)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MIsmo procedimiento pero con sustantivos\n",
    "def obtener_noun_synsets(noun):\n",
    "    return wn.synsets(noun, pos=wn.NOUN)\n",
    "\n",
    "def calcular_similitud_path(noun1, noun2):\n",
    "    synsets1 = obtener_noun_synsets(noun1)\n",
    "    synsets2 = obtener_noun_synsets(noun2)\n",
    "    max_similarity = 0\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            similarity = synset1.path_similarity(synset2)\n",
    "            if similarity and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    return max_similarity\n",
    "\n",
    "def encontrar_nouns_similares(most_common_noun, nouns):\n",
    "    similaridades = []\n",
    "    for noun in set(nouns):\n",
    "        if noun != most_common_noun:\n",
    "            similarity = calcular_similitud_path(most_common_noun, noun)\n",
    "            if similarity:  # Ignorar similaridades None\n",
    "                similaridades.append((noun, similarity))\n",
    "    similaridades.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similaridades[:5]\n",
    "\n",
    "for doc in corpus.fileids():\n",
    "    print(doc)\n",
    "    nouns = []\n",
    "    text = corpus.raw(doc)\n",
    "    tagged_text = tokenizar(text)\n",
    "    for word, tag in tagged_text: \n",
    "        if tag == 'NN' or tag == 'NNS' or tag == 'NNP' or tag == 'NNPS':\n",
    "            nouns.append(word)\n",
    "    #Obtener el sustantivo más común\n",
    "    counter = Counter(nouns)\n",
    "    most_common_noun = counter.most_common(1)\n",
    "    print(\"Most common noun: \",most_common_noun)\n",
    "    #Obtener los synsets del sustantivo más común\n",
    "    nouns_similares = encontrar_nouns_similares(most_common_noun[0][0], nouns)\n",
    "    print(nouns_similares, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud de verbos con métrica 'wup_similarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dreams - Henri Bergson.txt\n",
      "Most common verb:  [('dreaming', 4)]\n",
      "[('dream', 1.0), ('thought', 0.75), ('seen', 0.75), ('see', 0.75), ('think', 0.75)] \n",
      "\n",
      "Memoirs of Extraordinary Popular Delusions and the Madness of Crowds - Charles Mackay.txt\n",
      "Most common verb:  [('made', 30)]\n",
      "[('held', 1.0), ('established', 1.0), ('take', 1.0), ('created', 1.0), ('threw', 1.0)] \n",
      "\n",
      "Nerves and Common Sense - Annie Payson Call.txt\n",
      "Most common verb:  [('come', 3)]\n",
      "[('see', 0.8571428571428571), ('seeing', 0.8571428571428571), ('grow', 0.8), ('known', 0.75), ('suffer', 0.75)] \n",
      "\n",
      "Ten Thousand Dreams Interpreted - Gustavus Hindman Miller.txt\n",
      "Most common verb:  [('thought', 38)]\n",
      "[('mean', 1.0), ('intended', 1.0), ('thinking', 1.0), ('recall', 1.0), ('think', 1.0)] \n",
      "\n",
      "The Mind and the Brain - Alfred Binet.txt\n",
      "Most common verb:  [('thought', 5)]\n",
      "[('mean', 1.0), ('supposed', 1.0), ('consider', 1.0), ('meaning', 1.0), ('suppose', 1.0)] \n",
      "\n",
      "The Mind of the Child - William T. Preyer.txt\n",
      "Most common verb:  [('made', 15)]\n",
      "[('established', 1.0), ('take', 1.0), ('gave', 1.0), ('give', 1.0), ('getting', 1.0)] \n",
      "\n",
      "The Science of Human Nature A Psychology for Beginners - William Henry Pyle.txt\n",
      "Most common verb:  [('know', 20)]\n",
      "[('living', 1.0), ('live', 1.0), ('known', 1.0), ('take', 0.8888888888888888), ('taking', 0.8888888888888888)] \n",
      "\n",
      "The Trained Memory - Warren Hilton.txt\n",
      "Most common verb:  [('discovered', 8)]\n",
      "[('reveal', 1.0), ('learned', 1.0), ('seeing', 1.0), ('found', 1.0), ('seen', 1.0)] \n",
      "\n",
      "The Untroubled Mind - Herbert J. Hall.txt\n",
      "Most common verb:  [('worry', 9)]\n",
      "[('concerned', 1.0), ('fear', 0.6666666666666666), ('dealing', 0.5714285714285714), ('mean', 0.5), ('become', 0.5)] \n",
      "\n",
      "Unconscious Memory - Samuel Butler.txt\n",
      "Most common verb:  [('darwin', 12)]\n",
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MIsmo procedimiento pero con la similitud de Wup (Wu-Palmer)\n",
    "def obtener_verbo_synsets(verb):\n",
    "    return wn.synsets(verb, pos=wn.VERB)\n",
    "\n",
    "def calcular_similitud_wup(verb1, verb2):\n",
    "    synsets1 = obtener_verbo_synsets(verb1)\n",
    "    synsets2 = obtener_verbo_synsets(verb2)\n",
    "    max_similarity = 0\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            similarity = synset1.wup_similarity(synset2) # Calcular la similitud entre los synsets\n",
    "            if similarity and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    return max_similarity\n",
    "\n",
    "def encontrar_verbos_similares(most_common_verb, verbos):\n",
    "    similaridades = []\n",
    "    for verbo in set(verbos):\n",
    "        if verbo != most_common_verb:\n",
    "            similarity = calcular_similitud_wup(most_common_verb, verbo)\n",
    "            if similarity:  # Ignorar similaridades None\n",
    "                similaridades.append((verbo, similarity))\n",
    "    similaridades.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similaridades[:5]\n",
    "# Iterar sobre los documentos\n",
    "for doc in corpus.fileids():\n",
    "    print(doc)\n",
    "    verbos = []\n",
    "    text = corpus.raw(doc)\n",
    "    tagged_text = tokenizar(text)\n",
    "    for word, tag in tagged_text: \n",
    "        if tag == 'VB' or tag == 'VBD' or tag == 'VBG' or tag == 'VBN' or tag == 'VBP' or tag == 'VBZ':\n",
    "            verbos.append(word)\n",
    "    #Obtener el verbo más común\n",
    "    counter = Counter(verbos)\n",
    "    most_common_verb = counter.most_common(1)\n",
    "    print(\"Most common verb: \",most_common_verb)\n",
    "    #Obtener los synsets del verbo más común\n",
    "    verbos_similares= encontrar_verbos_similares(most_common_verb[0][0], verbos)\n",
    "    print(verbos_similares, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitid de sustantivos con 'wup_similarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dreams - Henri Bergson.txt\n",
      "Most common noun:  [('dream', 20)]\n",
      "[('ambition', 1.0), ('imagery', 0.9411764705882353), ('vision', 0.9333333333333333), ('desire', 0.9230769230769231), ('death', 0.8235294117647058)] \n",
      "\n",
      "Memoirs of Extraordinary Popular Delusions and the Madness of Crowds - Charles Mackay.txt\n",
      "Most common noun:  [('law', 124)]\n",
      "[('philosophy', 0.9473684210526315), ('principle', 0.9411764705882353), ('force', 0.9230769230769231), ('police', 0.9), ('office', 0.8888888888888888)] \n",
      "\n",
      "Nerves and Common Sense - Annie Payson Call.txt\n",
      "Most common noun:  [('habit', 7)]\n",
      "[('change', 0.8421052631578947), ('turn', 0.75), ('work', 0.75), ('act', 0.7142857142857143), ('trouble', 0.7058823529411765)] \n",
      "\n",
      "Ten Thousand Dreams Interpreted - Gustavus Hindman Miller.txt\n",
      "Most common noun:  [('dream', 171)]\n",
      "[('aspiration', 1.0), ('reverie', 0.9523809523809523), ('nightmare', 0.9473684210526315), ('imagination', 0.9411764705882353), ('vision', 0.9333333333333333)] \n",
      "\n",
      "The Mind and the Brain - Alfred Binet.txt\n",
      "Most common noun:  [('mind', 15)]\n",
      "[('idea', 1.0), ('opinion', 0.9333333333333333), ('theory', 0.8), ('content', 0.8), ('faith', 0.8)] \n",
      "\n",
      "The Mind of the Child - William T. Preyer.txt\n",
      "Most common noun:  [('child', 81)]\n",
      "[('infant', 0.9523809523809523), ('issue', 0.9473684210526315), ('girl', 0.9090909090909091), ('boy', 0.9090909090909091), ('parent', 0.8)] \n",
      "\n",
      "The Science of Human Nature A Psychology for Beginners - William Henry Pyle.txt\n",
      "Most common noun:  [('psychology', 48)]\n",
      "[('science', 0.9411764705882353), ('memory', 0.9), ('study', 0.875), ('field', 0.875), ('chemistry', 0.8421052631578947)] \n",
      "\n",
      "The Trained Memory - Warren Hilton.txt\n",
      "Most common noun:  [('experience', 11)]\n",
      "[('taste', 0.9230769230769231), ('life', 0.9230769230769231), ('flash', 0.9230769230769231), ('time', 0.9230769230769231), ('vision', 0.9230769230769231)] \n",
      "\n",
      "The Untroubled Mind - Herbert J. Hall.txt\n",
      "Most common noun:  [('life', 22)]\n",
      "[('spirit', 1.0), ('living', 1.0), ('time', 0.9333333333333333), ('experience', 0.9230769230769231), ('existence', 0.9090909090909091)] \n",
      "\n",
      "Unconscious Memory - Samuel Butler.txt\n",
      "Most common noun:  [('darwin', 31)]\n",
      "[('naturalist', 0.9523809523809523), ('wallace', 0.9090909090909091), ('lamarck', 0.9090909090909091), ('huxley', 0.8571428571428571), ('prague', 0.8181818181818182)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MIsmo procedimiento pero con sustantivos y wu palmer\n",
    "def obtener_noun_synsets(noun):\n",
    "    return wn.synsets(noun, pos=wn.NOUN)\n",
    "\n",
    "def calcular_similitud_wup(noun1, noun2):\n",
    "    synsets1 = obtener_noun_synsets(noun1)\n",
    "    synsets2 = obtener_noun_synsets(noun2)\n",
    "    max_similarity = 0\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            similarity = synset1.wup_similarity(synset2)\n",
    "            if similarity and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    return max_similarity\n",
    "\n",
    "def encontrar_nouns_similares(most_common_noun, nouns):\n",
    "    similaridades = []\n",
    "    for noun in set(nouns):\n",
    "        if noun != most_common_noun:\n",
    "            similarity = calcular_similitud_wup(most_common_noun, noun)\n",
    "            if similarity:  # Ignorar similaridades None\n",
    "                similaridades.append((noun, similarity))\n",
    "    similaridades.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similaridades[:5]\n",
    "\n",
    "for doc in corpus.fileids():\n",
    "    print(doc)\n",
    "    nouns = []\n",
    "    text = corpus.raw(doc)\n",
    "    tagged_text = tokenizar(text)\n",
    "    for word, tag in tagged_text: \n",
    "        if tag == 'NN' or tag == 'NNS' or tag == 'NNP' or tag == 'NNPS':\n",
    "            nouns.append(word)\n",
    "    #Obtener el sustantivo más común\n",
    "    counter = Counter(nouns)\n",
    "    most_common_noun = counter.most_common(1)\n",
    "    print(\"Most common noun: \",most_common_noun)\n",
    "    #Obtener los synsets del sustantivo más común\n",
    "    nouns_similares = encontrar_nouns_similares(most_common_noun[0][0], nouns)\n",
    "    print(nouns_similares, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similtud de documentos con synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dreams - Henri Bergson.txt\n",
      "Palabra clave más importante: essay professor bergson made several contributions \n",
      "\n",
      "Memoirs of Extraordinary Popular Delusions and the Madness of Crowds - Charles Mackay.txt\n",
      "Palabra clave más importante: que lass arriva dans notre bonne ville monsieur le r gent publia que lass serait utile pour r tablir la nation la faridondaine la faridondon mais il nous \n",
      "\n",
      "Nerves and Common Sense - Annie Payson Call.txt\n",
      "Palabra clave más importante: faith cure christian science mind cure hypnotism psychotherapeutics \n",
      "\n",
      "Ten Thousand Dreams Interpreted - Gustavus Hindman Miller.txt\n",
      "Palabra clave más importante: virile serpents cankerous lizards slimy intestine worms hairy \n",
      "\n",
      "The Mind and the Brain - Alfred Binet.txt\n",
      "Palabra clave más importante: hitherto dreamed first let us say \n",
      "\n",
      "The Mind of the Child - William T. Preyer.txt\n",
      "Palabra clave más importante: deaf mutes grunds tze und grundz ge zur aufstellung eines lehrplans f r eine taubstummen anstalt \n",
      "\n",
      "The Science of Human Nature A Psychology for Beginners - William Henry Pyle.txt\n",
      "Palabra clave más importante: certain well defined situations certain events always take place according \n",
      "\n",
      "The Trained Memory - Warren Hilton.txt\n",
      "Palabra clave más importante: human mind sir william hamilton quotes \n",
      "\n",
      "The Untroubled Mind - Herbert J. Hall.txt\n",
      "Palabra clave más importante: every one concerned large serenity must finally \n",
      "\n",
      "Unconscious Memory - Samuel Butler.txt\n",
      "Palabra clave más importante: grandfather dr erasmus darwin dr krause indeed thought otherwise \n",
      "\n",
      "['essay professor bergson made several contributions', 'que lass arriva dans notre bonne ville monsieur le r gent publia que lass serait utile pour r tablir la nation la faridondaine la faridondon mais il nous', 'faith cure christian science mind cure hypnotism psychotherapeutics', 'virile serpents cankerous lizards slimy intestine worms hairy', 'hitherto dreamed first let us say', 'deaf mutes grunds tze und grundz ge zur aufstellung eines lehrplans f r eine taubstummen anstalt', 'certain well defined situations certain events always take place according', 'human mind sir william hamilton quotes', 'every one concerned large serenity must finally', 'grandfather dr erasmus darwin dr krause indeed thought otherwise']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from rake_nltk import Rake\n",
    "# Se elige la primera frase clave de cada documento con RAKE-NLTK\n",
    "#Se carga el modelo de RAKE\n",
    "rake_nltk_var= Rake()\n",
    "# Función para obtener oraciones\n",
    "def oraciones(text):\n",
    "    lowercase_text = text.lower()\n",
    "    cleaned_text = re.sub(r'[^a-z]', ' ', lowercase_text) # Eliminar caracteres no alfabéticos\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) # Eliminar espacios en blanco\n",
    "    \n",
    "    sent_text = nltk.sent_tokenize(cleaned_text) # Obtener oraciones\n",
    "    return sent_text\n",
    "\n",
    "# Obtener la frase clave más importante de cada documento con RAKE\n",
    "frases = []\n",
    "# Iterar sobre los documentos\n",
    "for doc in corpus.fileids():\n",
    "    print(doc)\n",
    "    text = corpus.raw(doc)\n",
    "    sent_text = oraciones(text)\n",
    "    texto=' '.join(sent_text)\n",
    "    #Extraer las frases clave\n",
    "    rake_nltk_var.extract_keywords_from_text(texto)\n",
    "    #Obtener la frase clave más importante\n",
    "    keyword_extracted = rake_nltk_var.get_ranked_phrases()[0]\n",
    "    print('Frase más representativa:', keyword_extracted, \"\\n\")\n",
    "    # Guardar la frase clave\n",
    "    frases.append(keyword_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud con métrica 'path_similarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase base: certain well defined situations certain events always take place according\n",
      "\n",
      "Similitudes de la frase del documento 7 con las demás frases:\n",
      "Documento 1: Similitud = 0.3733\n",
      "Documento 2: Similitud = 0.3083\n",
      "Documento 3: Similitud = 0.3060\n",
      "Documento 4: Similitud = 0.2893\n",
      "Documento 5: Similitud = 0.2950\n",
      "Documento 6: Similitud = 0.2893\n",
      "Documento 8: Similitud = 0.3083\n",
      "Documento 9: Similitud = 0.2917\n",
      "Documento 10: Similitud = 0.3010\n"
     ]
    }
   ],
   "source": [
    "#Se obtiene los synsets de una palabra\n",
    "def obtener_synsets(palabra):\n",
    "    return wn.synsets(palabra)\n",
    "#Se calcula la similitud entre dos oraciones\n",
    "def calcular_similitud_oraciones(phrase1, phrase2):  \n",
    "    # Tokenizar las frases\n",
    "    words1 = phrase1.split()\n",
    "    words2 = phrase2.split()  \n",
    "    max_similarities = []\n",
    "    # Iterar sobre las palabras de la primera frase\n",
    "    for word1 in words1:\n",
    "        # Obtener los synsets de la palabra\n",
    "        synsets1 = obtener_synsets(word1)\n",
    "        max_similarity = 0\n",
    "        for word2 in words2:\n",
    "            # Obtener los synsets de la palabra de la segunda frase\n",
    "            synsets2 = obtener_synsets(word2)\n",
    "            for synset1 in synsets1:\n",
    "                for synset2 in synsets2:\n",
    "                    # Calcular la similitud entre los synsets\n",
    "                    similarity = synset1.path_similarity(synset2)\n",
    "                    if similarity and similarity > max_similarity:\n",
    "                        max_similarity = similarity\n",
    "        if max_similarity > 0:\n",
    "            # Guardar la similitud máxima\n",
    "            max_similarities.append(max_similarity)\n",
    "    # Calcular la similitud promedio\n",
    "    if max_similarities:\n",
    "        return sum(max_similarities) / len(max_similarities)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#La frase a comparar es la numero 7\n",
    "frase_base = frases[6]\n",
    "print(f\"Frase base: {frase_base}\\n\")\n",
    "similitudes = []\n",
    "# Iterar sobre las frases de los documentos\n",
    "for i, frase in enumerate(frases):\n",
    "    if i != 6:\n",
    "        # Calcular la similitud entre la frase base y la frase actual\n",
    "        similitud = calcular_similitud_oraciones(frase_base, frase)\n",
    "        similitudes.append((i+1, similitud))\n",
    "\n",
    "# Imprimir las similitudes\n",
    "print(f\"Similitudes de la frase del documento 7 con las demás frases:\")\n",
    "for doc_num, similitud in similitudes:\n",
    "    print(f\"Documento {doc_num}: Similitud = {similitud:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de palabras con embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dreams - Henri Bergson.txt\n",
      "Most common verb:  [('dreaming', 4)]\n",
      "Los 5 términos más similares a '[('dreaming', 4)]' son:\n",
      "Término: dreamed, Similitud: 0.5849594473838806\n",
      "Término: dreams, Similitud: 0.5801305770874023\n",
      "Término: dream, Similitud: 0.5788585543632507\n",
      "Término: waking, Similitud: 0.5402055978775024\n",
      "Término: dreamt, Similitud: 0.5081028938293457\n",
      "\n",
      "\n",
      "Memoirs of Extraordinary Popular Delusions and the Madness of Crowds - Charles Mackay.txt\n",
      "Most common verb:  [('made', 30)]\n",
      "Los 5 términos más similares a '[('made', 30)]' son:\n",
      "Término: making, Similitud: 0.8200170397758484\n",
      "Término: make, Similitud: 0.7930080890655518\n",
      "Término: but, Similitud: 0.6358116865158081\n",
      "Término: came, Similitud: 0.6152335405349731\n",
      "Término: that, Similitud: 0.6122232675552368\n",
      "\n",
      "\n",
      "Nerves and Common Sense - Annie Payson Call.txt\n",
      "Most common verb:  [('come', 3)]\n",
      "Los 5 términos más similares a '[('come', 3)]' son:\n",
      "Término: coming, Similitud: 0.8334023952484131\n",
      "Término: go, Similitud: 0.7987576127052307\n",
      "Término: n't, Similitud: 0.7807924151420593\n",
      "Término: we, Similitud: 0.7733871340751648\n",
      "Término: going, Similitud: 0.7716752886772156\n",
      "\n",
      "\n",
      "Ten Thousand Dreams Interpreted - Gustavus Hindman Miller.txt\n",
      "Most common verb:  [('thought', 38)]\n",
      "Los 5 términos más similares a '[('thought', 38)]' son:\n",
      "Término: think, Similitud: 0.7623668909072876\n",
      "Término: something, Similitud: 0.7589041590690613\n",
      "Término: what, Similitud: 0.7577835917472839\n",
      "Término: probably, Similitud: 0.7480263113975525\n",
      "Término: actually, Similitud: 0.7460173964500427\n",
      "\n",
      "\n",
      "The Mind and the Brain - Alfred Binet.txt\n",
      "Most common verb:  [('thought', 5)]\n",
      "Los 5 términos más similares a '[('thought', 5)]' son:\n",
      "Término: think, Similitud: 0.7623669505119324\n",
      "Término: something, Similitud: 0.7589041590690613\n",
      "Término: what, Similitud: 0.7577835917472839\n",
      "Término: probably, Similitud: 0.7480263113975525\n",
      "Término: actually, Similitud: 0.7460173964500427\n",
      "\n",
      "\n",
      "The Mind of the Child - William T. Preyer.txt\n",
      "Most common verb:  [('made', 15)]\n",
      "Los 5 términos más similares a '[('made', 15)]' son:\n",
      "Término: making, Similitud: 0.8200170397758484\n",
      "Término: make, Similitud: 0.7930080890655518\n",
      "Término: but, Similitud: 0.6358116865158081\n",
      "Término: came, Similitud: 0.6152335405349731\n",
      "Término: that, Similitud: 0.6122232675552368\n",
      "\n",
      "\n",
      "The Science of Human Nature A Psychology for Beginners - William Henry Pyle.txt\n",
      "Most common verb:  [('know', 20)]\n",
      "Los 5 términos más similares a '[('know', 20)]' son:\n",
      "Término: think, Similitud: 0.8562581539154053\n",
      "Término: n't, Similitud: 0.8541332483291626\n",
      "Término: really, Similitud: 0.854002833366394\n",
      "Término: what, Similitud: 0.846484899520874\n",
      "Término: why, Similitud: 0.842064619064331\n",
      "\n",
      "\n",
      "The Trained Memory - Warren Hilton.txt\n",
      "Most common verb:  [('discovered', 8)]\n",
      "Los 5 términos más similares a '[('discovered', 8)]' son:\n",
      "Término: found, Similitud: 0.6860091090202332\n",
      "Término: unearthed, Similitud: 0.6030711531639099\n",
      "Término: uncovered, Similitud: 0.588854193687439\n",
      "Término: discovering, Similitud: 0.5817255973815918\n",
      "Término: discovery, Similitud: 0.5648726224899292\n",
      "\n",
      "\n",
      "The Untroubled Mind - Herbert J. Hall.txt\n",
      "Most common verb:  [('worry', 9)]\n",
      "Los 5 términos más similares a '[('worry', 9)]' son:\n",
      "Término: worried, Similitud: 0.8061186671257019\n",
      "Término: fear, Similitud: 0.7108753323554993\n",
      "Término: worries, Similitud: 0.688184380531311\n",
      "Término: wo, Similitud: 0.6648355722427368\n",
      "Término: concern, Similitud: 0.6623204946517944\n",
      "\n",
      "\n",
      "Unconscious Memory - Samuel Butler.txt\n",
      "Most common verb:  [('darwin', 12)]\n",
      "Los 5 términos más similares a '[('darwin', 12)]' son:\n",
      "Término: naturalist, Similitud: 0.43439000844955444\n",
      "Término: lyell, Similitud: 0.4255163073539734\n",
      "Término: adelaide, Similitud: 0.4160202741622925\n",
      "Término: huxley, Similitud: 0.4122277498245239\n",
      "Término: galton, Similitud: 0.40801119804382324\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Cargar el modelo GloVe\n",
    "glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "# Función para obtener los términos más similares a una palabra con GloVe\n",
    "def obtener_terminos_similares(glove_model, word, top_n=5):\n",
    "    try:\n",
    "        similar_terms = glove_model.most_similar(word, topn=top_n)\n",
    "        return similar_terms\n",
    "    except KeyError:\n",
    "        return []\n",
    "# Iterar sobre los documentos\n",
    "for doc in corpus.fileids():\n",
    "    print(doc)\n",
    "    verbos = []\n",
    "    text = corpus.raw(doc)\n",
    "    tagged_text = tokenizar(text)\n",
    "    #Obtener los verbos\n",
    "    for word, tag in tagged_text: \n",
    "        if tag == 'VB' or tag == 'VBD' or tag == 'VBG' or tag == 'VBN' or tag == 'VBP' or tag == 'VBZ':\n",
    "            verbos.append(word)\n",
    "    #Obtener el verbo más común\n",
    "    counter = Counter(verbos)\n",
    "    most_common_verb = counter.most_common(1)\n",
    "    print(\"Most common verb: \",most_common_verb)\n",
    "    #Obtener los términos similares del verbo más común\n",
    "    if most_common_verb:\n",
    "        # Obtener los términos más similares\n",
    "        similares = obtener_terminos_similares(glove_model, most_common_verb, top_n=5)\n",
    "        print(f\"Los 5 términos más similares a '{most_common_verb}' son:\")\n",
    "        for termino, similitud in similares:\n",
    "            print(f\"Término: {termino}, Similitud: {similitud}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de documentos con embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre el documento 7 y el documento 1: 0.8381\n",
      "Similitud entre el documento 7 y el documento 2: 0.7777\n",
      "Similitud entre el documento 7 y el documento 3: 0.8196\n",
      "Similitud entre el documento 7 y el documento 4: 0.7747\n",
      "Similitud entre el documento 7 y el documento 5: 0.8654\n",
      "Similitud entre el documento 7 y el documento 6: 0.8007\n",
      "Similitud entre el documento 7 y el documento 8: 0.8829\n",
      "Similitud entre el documento 7 y el documento 9: 0.8580\n",
      "Similitud entre el documento 7 y el documento 10: 0.8742\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cargar el tokenizer y el modelo BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def obtener_embedding(frase):\n",
    "    # Tokenizar la frase y convertirla a tensores de entrada\n",
    "    inputs = tokenizer(frase, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Obtener la salida de BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Obtener el embedding de la [CLS] token\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    \n",
    "    return cls_embedding\n",
    "\n",
    "# Obtener los embeddings de las frases\n",
    "embeddings = [obtener_embedding(frase) for frase in frases]\n",
    "\n",
    "# Embedding del documento 7 (índice 6 si se usa una lista 0-indexada)\n",
    "embedding_doc7 = embeddings[6]\n",
    "\n",
    "# Calcular la similitud de coseno entre el documento 7 y los demás documentos\n",
    "similitud_doc7 = cosine_similarity([embedding_doc7], embeddings)\n",
    "\n",
    "# Imprimir las similitudes\n",
    "for i, similitud in enumerate(similitud_doc7[0]):\n",
    "    if i == 6:\n",
    "        continue  # Saltar la comparación consigo mismo\n",
    "    print(f\"Similitud entre el documento 7 y el documento {i + 1}: {similitud:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
